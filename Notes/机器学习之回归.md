## 机器学习之线性回归

## 应用举例

* 股市预测
* 房价预测

##  模型步骤

* 模型假设，选择模型框架	（线性模型）
* 模型评估，判断众多模型的好坏   （损失函数）
* 模型优化 ，筛选最优的模型      （梯度下降）

#### 模型假设

##### 线性模型

###### 一元线性模型

只有一个特征		*y*=*b*+w·x

###### 多元线性模型

有多个特征

![多元线性模型](C:\Users\xiaobo\AppData\Roaming\Typora\typora-user-images\image-20191205224435904.png)

#### 模型评估

损失函数
$$
L(w,b) = \sum_{n=1}^{10}{(y^n - (b+w*x_{cp}))^2}
$$


常见的有：

* 平方
* 绝对值



#### 模型优化

采用梯度下降法

步骤

1. 随机选取一个 w

2. 计算当前斜率，根据斜率来判定移动方向

   ​	大于零右移，小于零左移

3. 根据学习率移动

4. 重复2，3 直至最低点

<img src="https://datawhalechina.github.io/leeml-notes/chapter3/res/chapter3-9.png" alt="图解步骤" style="zoom:50%;" />



​	w和b偏微分的计算方法

​	<img src="https://datawhalechina.github.io/leeml-notes/chapter3/res/chapter3-14.png" alt="计算方法" style="zoom:50%;" />

#### 如何验证模型好坏



#### 如何优化

* 正则化
* 更多参数
* 减少参数



##### 加入正则化

$$
 y = b + \sum{w_ix_i}
$$

$$
L = \sum_n{(y^n - (b + \sum{w_ix_i}))^2} + \lambda\sum{(w_i)^2} 
$$



![正则化](https://datawhalechina.github.io/leeml-notes/chapter3/res/chapter3-29.png)



## 自适应学习率

###  Adagrad算法

每个参数的学习率都把它除上之前微分的均方根。



<img src="https://datawhalechina.github.io/leeml-notes/chapter6/res/chapter6-4.png" alt="img" style="zoom:50%;" />



化简

![img](https://datawhalechina.github.io/leeml-notes/chapter6/res/chapter6-5.png)



## 随机梯度下降

损失函数不需要处理训练集所有的数据

Before:
$$
L(w,b) = \sum_{n=1}^{10}{(y^n - (b+w*x_{cp}))^2}
$$

$$
\theta^i = \theta^{i-1} - \eta\nabla L(\theta^{i-1})
$$

After:
$$
L(w,b) = \sum_{n=1}^{10}{(y^n - (b+w*x_{cp}))^2}
$$

$$
<Empty \space Math \space Block>
$$

$$
\theta^i = \theta^{i-1} - \eta\nabla L^n(\theta^{i-1})
$$

此时不需要像之前那样对所有的数据进行处理，只需要计算某一个例子的损失函数Ln，就可以赶紧update 梯度。



### 梯度下降的理论基础

泰勒展开推导



## 特征放缩

常用方法：
$$
	X_i^r = \frac{X_i^r - m_i}{\sigma_i}
$$
 (X - 平均值) 除以标准差



