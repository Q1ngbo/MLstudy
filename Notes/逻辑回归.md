# 逻辑回归

作用：解决分类问题



## Sigmoid function

映射函数 Sigmoid function
$$
g(z) = \frac{1}{1+e^{-z}}
$$

## Cost function

$$
J(\theta) = \frac{1}{m}\sum_{i=1}^{m}{Cost(h_\theta(x^{(i)}),y^{(i)})}
$$

$$

Cost(h_\theta(x),y) =  \begin{cases} \ -log(h_\theta(x)) \quad\quad\quad if\quad y = 1 \\
\ -log(1-h_\theta(x)) \quad if \quad y = 0
\end{cases}
$$


$$
J(\theta) = \frac{1}{m} \sum_{i=1}^{m}{[-y^{(i)}log(h_\theta(x^{(i)})) - (1-y^{(i)}) log(1-h_\theta(x^{(i)}))]}
$$
又等于
$$
J(\theta) = -\frac{1}{m} \sum_{i=1}^{m}{[y^{(i)}log(h_\theta(x^{(i)})) + (1-y^{(i)}) log(1-h_\theta(x^{(i)}))]}
$$


## Gradient Descent

$$
\theta_{j} = \theta_{j} - \alpha\frac{1}{m}\sum_{i=1}^m{(h_{\theta}(x^{(i)}) - y^{(i)})x_{j}^{(i)}}
$$

Simultaneously



# Regularization

如果出现了过拟合问题，应该如何处理

* 丢弃一些不能帮助我们正确预测的特征 可以手工选择 也可以选择使用PCA算法
* 正则化 保留全部特征，但是减少参数的大小





### 正则化线性回归

$$
J(\theta) = \frac{1}{2m}[\sum_{i=1}^{m}{(h_\theta(x^{(i)}) - y^{(i)})^2 + \lambda \sum_{j=1}^{n}{\theta_j^2}}]
$$

$$
\theta_0 = \theta_0 - \alpha\frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})  - y^{(i)})x_0^{(i)}
$$

$$
\theta_j = \theta_j - \alpha[\frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})  - y^{(i)})x_j^{(i)} + \frac{\lambda}{m}\theta_j]
$$

$$
\theta_j = \theta_j(1-\alpha\frac{\lambda}{m}) -  \alpha\frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})  - y^{(i)})x_j^{(i)}
$$

### 正则化逻辑回归

$$
J(\theta) = \frac{1}{m} \sum_{i=1}^{m}{[-y^{(i)}log(h_\theta(x^{(i)})) - (1-y^{(i)}) log(1-h_\theta(x^{(i)}))]} + \frac{\lambda}{2m}\sum_{j=1}^n{\theta_j^2}
$$

